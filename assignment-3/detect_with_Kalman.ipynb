{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/Jasonchen0317/CSGY-6613-Assignment/blob/main/assignment-3/detect_with_Kalman.ipynb\">\n",
    "  <img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2: Kalman Filter \n",
    "\n",
    "This task aim to implement a Kalman filter that will track the drone in the video.\n",
    "\n",
    "### Approach\n",
    "\n",
    "Most part of task 2 is based on detect.py from task 1, since the output of the detection model will be used as the measurement for the kalman filter. \n",
    "\n",
    "#### Kalman filter instance variable\n",
    "\n",
    "1. Filter state estimate(x) is composed by coordinates of the center of the bounding box (cx,cy), size of the box (width, height) and the change of each of these parameters, velocities.\n",
    "\n",
    "2. Covariance matrix(P) is set to I*10 (np.eye(8) * 10.).\n",
    "\n",
    "3. Process uncertainty/noise(Q) is set to I*0.01 (np.eye(8) * 0.01).\n",
    "\n",
    "4. Measurement uncertainty/noise(R) is set 1 for center point, 10 for width and height.\n",
    "\n",
    "#### Steps\n",
    "\n",
    "First, the bounding box is found by the Faster-RCNN model, then is converted into the format of the measurement for the filter. The filter first predicts and then uses the measurement to update the resulting bounding box. \n",
    "\n",
    "For small amount(<20) of consecutive frames that has no detection, I've used the kalman filter to predict the bounding boxes, since it maybe false negative(miss detection). If the amount of consecutive frames that has no detection is over the threshold, the saved detected frames will be output as videos. \n",
    "\n",
    "#### Results\n",
    "\n",
    "The resulting videos are saved in the 'kalman_filter_vid' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import glob as glob\n",
    "import os\n",
    "import time\n",
    "from model import create_model\n",
    "from config import (\n",
    "    NUM_CLASSES, DEVICE, CLASSES\n",
    ")\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from filterpy.common import Q_discrete_white_noise\n",
    "from scipy.linalg import block_diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for creating the kalman filter\n",
    "def tracker():\n",
    "    #Dimension of state is 8, and dim for measurement is 4.\n",
    "    tracker = KalmanFilter(dim_x=8, dim_z=4)\n",
    "    dt = 1.0   # time step\n",
    "    #x is composed by coordinates of the center of the bounding box (cx,cy), size of the box (width, height) and the change of each of these parameters, velocities.\n",
    "    #State Transition: x1=x0+dt, y1=y0+dt\n",
    "    tracker.F = np.array([[1, 0, 0, 0, dt, 0, 0, 0],\n",
    "                          [0, 1, 0, 0, 0, dt, 0, 0],\n",
    "                          [0, 0, 1, 0, 0, 0, dt, 0],\n",
    "                          [0, 0, 0, 1, 0, 0, 0, dt],\n",
    "                          [0, 0, 0, 0, 1, 0, 0, 0],\n",
    "                          [0, 0, 0, 0, 0, 1, 0, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 1, 0],\n",
    "                          [0, 0, 0, 0, 0, 0, 0, 1]])\n",
    "    tracker.u = 0.\n",
    "    tracker.H = np.array([[1, 0, 0, 0, 0, 0, 0, 0],\n",
    "                          [0, 1, 0, 0, 0, 0, 0, 0],\n",
    "                          [0, 0, 1, 0, 0, 0, 0, 0],\n",
    "                          [0, 0, 0, 1, 0, 0, 0, 0]])\n",
    "\n",
    "    tracker.R = np.array([[1, 0, 0, 0],\n",
    "                          [0, 1, 0, 0],\n",
    "                          [0, 0, 10, 0],\n",
    "                          [0, 0, 0, 10]])\n",
    "    q = Q_discrete_white_noise(dim=2, dt=dt, var=0.05)\n",
    "    tracker.Q = np.eye(8) * 0.01\n",
    "    tracker.x = np.array([[0, 0, 0, 0, 0, 0, 0, 0]]).T\n",
    "    tracker.P = np.eye(8) * 10.\n",
    "    return tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for converting bounding boxes to measurement format for kalman filter\n",
    "#bbox:[x1, y1, x2, y2] ----> measurement(z): [center x, center y, width, height]\n",
    "def bboxes_to_z(bbox):\n",
    "    w = bbox[2]-bbox[0]\n",
    "    h = bbox[3] - bbox[1]\n",
    "    cx = (bbox[2]+bbox[0])/2\n",
    "    cy = (bbox[3]+bbox[1])/2\n",
    "    return np.array([cx, cy, w, h]).reshape((4, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for converting result(x) to bounding box format and add center point\n",
    "#result(x): [center x, center y, width, height] ----> bbox: [x1, y1, x2, y2, center x, center y]\n",
    "def x_to_bbox(x):\n",
    "    x1 = x[0]-x[2]/2\n",
    "    x2 = x[0]+x[2]/2\n",
    "    y1 = x[1]-x[3]/2\n",
    "    y2 = x[1]+x[3]/2\n",
    "    #return [topleft x, topleft y, bottomright x, bottomright y, center x, center y]\n",
    "    return np.array([x1, y1, x2, y2, x[0], x[1]]).reshape((6, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for converting saved frames to videos\n",
    "def outputvideo(img_arr, num):\n",
    "    img = img_arr[0]\n",
    "    height, width, layers = img.shape\n",
    "    size = (width,height)\n",
    "            \n",
    "    out = cv2.VideoWriter('C:/Users/jason/CSGY-6613-Assignment/assignment-3/kalman_filter_vid/detect_video_%d.mp4' % num,cv2.VideoWriter_fourcc(*'mp4v'), 15, size)\n",
    "    \n",
    "            \n",
    "    for i in range(len(img_arr)):\n",
    "         out.write(img_arr[i])\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('kalman_filter_vid'):\n",
    "   os.makedirs('kalman_filter_vid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = (0, 255, 0)\n",
    "# load the best model and trained weights\n",
    "model = create_model(num_classes=NUM_CLASSES)\n",
    "checkpoint = torch.load('outputs/best_model.pth', map_location=DEVICE)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.to(DEVICE).eval()\n",
    "\n",
    "# directory where all the images are present\n",
    "DIR_TEST = 'dataset/test'\n",
    "test_images = glob.glob(f\"{DIR_TEST}/*.jpg\")\n",
    "print(f\"Test instances: {len(test_images)}\")\n",
    "\n",
    "#Threshold for bounding boxes, if lower then threshold, the bbox isn't valid\n",
    "detection_threshold=0.5\n",
    "#The Kalman filter for tracking\n",
    "trk = tracker()\n",
    "#List for saving the centers of bounding boxes, for plotting trajectories\n",
    "centers=[]\n",
    "#List for saving frames that detected drones\n",
    "img_arr=[]\n",
    "#Counter for consecutive frames without detection\n",
    "miss_count=0\n",
    "#Counter for currently saved videos\n",
    "video_num=0\n",
    "\n",
    "for i in range(len(test_images)):\n",
    "    # get the image file name for saving output later on\n",
    "    image_name = test_images[i].split(os.path.sep)[-1].split('.')[0]\n",
    "    image = cv2.imread(test_images[i])\n",
    "    orig_image = image.copy()\n",
    "    # BGR to RGB\n",
    "    image = cv2.cvtColor(orig_image, cv2.COLOR_BGR2RGB).astype(np.float32)\n",
    "    # make the pixel range between 0 and 1\n",
    "    image /= 255.0\n",
    "    # bring color channels to front\n",
    "    image = np.transpose(image, (2, 0, 1)).astype(np.float32)\n",
    "    # convert to tensor\n",
    "    image = torch.tensor(image, dtype=torch.float).cuda()\n",
    "    # add batch dimension\n",
    "    image = torch.unsqueeze(image, 0)\n",
    "    start_time = time.time()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(image.to(DEVICE))\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # load all detection to CPU for further operations\n",
    "    outputs = [{k: v.to('cpu') for k, v in t.items()} for t in outputs]\n",
    "    \n",
    "    # carry further only if there are detected boxes\n",
    "    if len(outputs[0]['boxes']) != 0:\n",
    "        boxes = outputs[0]['boxes'].data.numpy()\n",
    "        scores = outputs[0]['scores'].data.numpy()\n",
    "        # get the bbox with the highest confidence\n",
    "        max_score = np.argmax(scores)\n",
    "        \n",
    "        #Check if the confidence is higher then the threshold\n",
    "        #If higher, proceed to apply Kalman filter\n",
    "        if(scores[max_score]>detection_threshold):\n",
    "            miss_count=0\n",
    "            trk.predict()\n",
    "            trk.update(bboxes_to_z(boxes[max_score]))\n",
    "            box = x_to_bbox(trk.x).astype(np.int32)\n",
    "            centers.append((int(box[4]), int(box[5])))\n",
    "        #Lower and less then 20 consecutive frames without detection, predict the bounding box by previous detections\n",
    "        elif miss_count<20:\n",
    "            miss_count = miss_count+1\n",
    "            trk.predict()\n",
    "            box = x_to_bbox(trk.x).astype(np.int32)\n",
    "            centers.append((int(box[4]), int(box[5])))\n",
    "        \n",
    "        #If more 20 consecutive frames without detection, output the saved frames to video\n",
    "        else:\n",
    "            #If detected more than 50 detected frames, convert and output as video\n",
    "            if len(img_arr)>50:\n",
    "                outputvideo(img_arr, video_num)\n",
    "                video_num=video_num+1\n",
    "            #Clear frame list and center list    \n",
    "            img_arr=[]\n",
    "            centers=[]\n",
    "            continue\n",
    "            \n",
    "        # draw the trajectory, bounding boxes, and class name\n",
    "        class_name = 'drone'\n",
    "        if(len(centers)>=2):\n",
    "            for i in range(len(centers)-1):\n",
    "                cv2.line(orig_image,centers[i],centers[i+1],(255,0,0),3)\n",
    "                \n",
    "        cv2.rectangle(orig_image,\n",
    "                        (int(box[0]), int(box[1])),\n",
    "                        (int(box[2]), int(box[3])),\n",
    "                        color, 2)\n",
    "        cv2.putText(orig_image, class_name, \n",
    "                        (int(box[0]), int(box[1]-5)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, \n",
    "                        2, lineType=cv2.LINE_AA)\n",
    "        cv2.imshow('Prediction', orig_image)\n",
    "        cv2.waitKey(1)\n",
    "        #Keep frame in list\n",
    "        img_arr.append(orig_image)\n",
    "    \n",
    "    #If less then 20 consecutive frames without detection, predict the bounding box by previous detections \n",
    "    elif miss_count<20:\n",
    "        miss_count = miss_count+1\n",
    "        trk.predict()\n",
    "        box = x_to_bbox(trk.x).astype(np.int32)\n",
    "        \n",
    "        # draw the bounding boxes and write the class name on top of it\n",
    "        class_name = 'drone'\n",
    "        centers.append((int(box[4]), int(box[5])))\n",
    "        \n",
    "        if(len(centers)>=2):\n",
    "            for i in range(len(centers)-1):\n",
    "                cv2.line(orig_image,centers[i],centers[i+1],(255,0,0),3)\n",
    "                \n",
    "        cv2.rectangle(orig_image,\n",
    "                        (int(box[0]), int(box[1])),\n",
    "                        (int(box[2]), int(box[3])),\n",
    "                        color, 2)\n",
    "        cv2.putText(orig_image, class_name, \n",
    "                        (int(box[0]), int(box[1]-5)),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.7, color, \n",
    "                        2, lineType=cv2.LINE_AA)\n",
    "        cv2.imshow('Prediction', orig_image)\n",
    "        cv2.waitKey(1)\n",
    "        img_arr.append(orig_image)\n",
    "    \n",
    "    #If more 20 consecutive frames without detection, output the saved frames to video\n",
    "    else:\n",
    "        if len(img_arr)>50:\n",
    "            outputvideo(img_arr, video_num)\n",
    "            video_num=video_num+1\n",
    "                \n",
    "        img_arr=[]\n",
    "        centers=[]\n",
    "        \n",
    "print('TEST PREDICTIONS COMPLETE')\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
